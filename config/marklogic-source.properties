# Kafka-specific properties

name=marklogic-source
connector.class=com.marklogic.kafka.connect.source.MarkLogicSourceConnector

# Should only need one task since it's using a WriteBatcher, which is multi-threaded
tasks.max=1

# Topic to push to
topic=marklogic-source


# MarkLogic connector-specific properties

# A MarkLogic host to connect to. The connector uses the Data Movement SDK, and thus it will connect to each of the
# hosts in a cluster.
ml.connection.host=localhost

# The port of a REST API server to connect to.
ml.connection.port=8000

# The query to find documents for export.
ml.query=

# Required - a collection that is used to limit the query results.
ml.query.collection=some-collection

# Required - a collection that replaces the source-collection to indicate the document has been exported.
ml.query.processed-collection=processed-collection

# Optional - the name of a database to connect to. If your REST API server has a content database matching that of the
# one that you want to write documents to, you do not need to set this.
ml.connection.database=Kafka

# Optional - set to "gateway" when using a load balancer, else leave blank. See https://docs.marklogic.com/guide/java/data-movement#id_26583 for more information.
ml.connection.type=

# Either DIGEST, BASIC, CERTIFICATE, KERBEROS, or NONE
ml.connection.securityContextType=DIGEST

# Set these based on the security context type defined above
ml.connection.username=admin
ml.connection.password=admin
ml.connection.certFile=
ml.connection.certPassword=
ml.connection.externalName=

# Set to "true" for a "simple" SSL strategy that uses the JVM's default SslContext and X509TrustManager and a
# "trust everything" HostnameVerifier. Further customization of an SSL connection via properties is not supported. If
# you need to do so, consider using the source code for this connector as a starting point.
ml.connection.simpleSsl=false

# Sets the number of documents to be written in a batch to MarkLogic. This may not have any impact depending on the
# connector receives data from Kafka, as the connector calls flushAsync on the DMSDK WriteBatcher after processing every
# collection of records. Thus, if the connector never receives at one time more than the value of this property, then
# the value of this property will have no impact.
ml.dmsdk.batchSize=100

# Sets the number of threads used by the Data Movement SDK for parallelizing writes to MarkLogic. Similar to the batch
# size property above, this may never come into play depending on how many records the connector receives at once.
ml.dmsdk.threadCount=8

# Optional - specify the format of each document; either JSON, XML, BINARY, TEXT, or UNKNOWN
ml.document.format=JSON

# Optional - specify a mime type for each document; typically the format property above will be used instead of this
ml.document.mimeType=

# Optional - a comma-separated list of roles and capabilities that define the permissions for each document written to MarkLogic
ml.document.permissions=rest-reader,read,rest-writer,update

# Optional - a prefix to prepend to each URI; the URI itself is a UUID
ml.document.uriPrefix=/kafka-data/

# Optional - a suffix to append to each URI
ml.document.uriSuffix=.json

ml.dmsdk.transform=
ml.dmsdk.transformParams=
