# Kafka-specific properties

name=marklogic-sink
connector.class=com.marklogic.kafka.connect.sink.MarkLogicSinkConnector

# Should only need one task since it's using a WriteBatcher, which is multi-threaded
tasks.max=1

# Topics to consume from [comma separated list for multiple topics]
topics=marklogic


# MarkLogic connector-specific properties

# A MarkLogic host to connect to. The connector uses the Data Movement SDK, and thus it will connect to each of the
# hosts in a cluster.
ml.connection.host=localhost

# The port of a REST API server to connect to.
ml.connection.port=8000

# Optional - the name of a database to connect to. If your REST API server has a content database matching that of the
# one that you want to write documents to, you do not need to set this.
ml.connection.database=Documents

# Optional - set to "gateway" when using a load balancer, else leave blank.
# See https://docs.marklogic.com/guide/java/data-movement#id_26583 for more information.
ml.connection.type=

# Either DIGEST, BASIC, CERTIFICATE, KERBEROS, or NONE
ml.connection.securityContextType=DIGEST

# Set these based on the security context type defined above
ml.connection.username=admin
ml.connection.password=admin
ml.connection.certFile=
ml.connection.certPassword=
ml.connection.externalName=

# Set "ml.connection.simpleSsl" to "true" for a "simple" SSL strategy that uses the JVM's default SslContext and
# X509TrustManager and a "trust everything" HostnameVerifier. Further customization of an SSL connection via properties
# is not supported. If you need to do so, consider using the source code for this connector as a starting point.
ml.connection.simpleSsl=false
# You must also ensure that the server cert or the signing CA cert is imported in the JVMs cacerts file.
# These commands may be used to get the server cert and to import it into your cacerts file.
# Don't forget to customize the commands for your particular case.
#   openssl x509 -in <(openssl s_client -connect <server>:8004 -prexit 2>/dev/null) -out ~/example.crt
#   sudo keytool -importcert -file ~/example.crt -alias <server> -keystore /path/to/java/lib/security/cacerts -storepass <storepass-password>

# Sets the number of documents to be written in a batch to MarkLogic. This may not have any impact depending on the
# connector receives data from Kafka, as the connector calls flushAsync on the DMSDK WriteBatcher after processing every
# collection of records. Thus, if the connector never receives at one time more than the value of this property, then
# the value of this property will have no impact.
ml.dmsdk.batchSize=100

# Sets the number of threads used by the Data Movement SDK for parallelizing writes to MarkLogic. Similar to the batch
# size property above, this may never come into play depending on how many records the connector receives at once.
ml.dmsdk.threadCount=8

# Optional - a comma-separated list of collections that each document should be written to
ml.document.collections=kafka-data

# Optional - set this to true so that the name of the topic that the connector reads from is added as a collection to each document inserted by the connector
ml.document.addTopicToCollections=false

# Optional - specify the format of each document; either JSON, XML, BINARY, TEXT, or UNKNOWN
ml.document.format=JSON

# Optional - specify a mime type for each document; typically the format property above will be used instead of this
ml.document.mimeType=

# Optional - a comma-separated list of roles and capabilities that define the permissions for each document written to MarkLogic
ml.document.permissions=rest-reader,read,rest-writer,update

# Optional - a prefix to prepend to each URI; the URI itself is a UUID
ml.document.uriPrefix=/kafka-data/

# Optional - a suffix to append to each URI
ml.document.uriSuffix=.json

# Optional - name of a REST transform to use when writing documents
# For Data Hub, can use mlRunIngest
ml.dmsdk.transform=

# Optional - delimited set of transform names and values
# Data Hub example = flow-name,ingestion_mapping_mastering-flow,step,1
ml.dmsdk.transformParams=

# Optional - delimiter for transform parameter names and values
ml.dmsdk.transformParamsDelimiter=,

# Properties for running a Data Hub flow
# Using examples/dh-5-example in the DH project, could use the following config:
# ml.datahub.flow.name=ingestion_mapping_mastering-flow
# ml.datahub.flow.steps=2,3,4
ml.datahub.flow.name=
ml.datahub.flow.steps=
# Whether or not the response data from running a flow should be logged at the info level
ml.datahub.flow.logResponse=true

# Logging configurations
ml.log.record.key=false
ml.log.record.headers=false

ml.id.strategy=
ml.id.strategy.paths=
ml.connection.enableCustomSsl=false
ml.connection.customSsl.tlsVersion=
ml.connection.customSsl.hostNameVerifier=
ml.connection.customSsl.mutualAuth=false
